---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# lessLM

The sole purpose of this package is to demonstrate how the optimizers implemented in [lessSEM](https://github.com/jhorzek/lessSEM) can be used by other packages. To this end, we use a fairly simple model: A
linear regression of the form

$$\pmb y = \pmb X \pmb b + \pmb\varepsilon$$

## Step 1

Install [lessSEM](https://github.com/jhorzek/lessSEM) from CRAN or [GitHub](https://github.com/jhorzek/lessSEM).

```{r,eval=FALSE}
install.packages("lessSEM")
```


## Step 2

Create a new R package which uses [RcppArmadillo](https://github.com/RcppCore/RcppArmadillo) (e.g., with `RcppArmadillo::RcppArmadillo.package.skeleton()`). Open the DESCRIPTION file and add lessSEM to the "LinkingTo" field (see the DESCRIPTION file of this package if you are unsure what we are referring to). If you already have a package, just add lessSEM to the "LinkingTo" field.

Alternatively, you can copy the optimizers from [here](https://github.com/jhorzek/lessOptimizers)
into the source folder of your package.

## Step 3: Implementing your package

We will assume that you already have a package which implements the model that you would like to regularize. Therefore, we will not make use of any of the functions and classes defined in [lessSEM](https://github.com/jhorzek/lessSEM) at this point. However, make sure that you have two functions:

1) A function which computes the fit (e.g., -2-log-Likelihood) of your model. This function must return a double.
2) A function which computes the gradients of your model. This function must return an arma::rowvec.

For our example, we have implemented these functions in the files [src/linearRegressionModel.h](https://github.com/jhorzek/lessLM/blob/main/src/linearRegressionModel.h) and [src/linearRegressionModel.cpp](https://github.com/jhorzek/lessLM/blob/main/src/linearRegressionModel.cpp). We also implemented a function to compute the Hessian. This function is very helpful when using the glmnet optimizer which does use an approximation of the Hessian based on the BFGS procedure. If the initial Hessian is a poor substitute for the true Hessian, this optimizer can return wrong parameter estimates. To get a reasonable initial Hessian, we therefore also implemented this initial Hessian estimation based on the procedure used in [lavaan](https://github.com/yrosseel/lavaan).

## Step 4: Linking to [lessSEM](https://github.com/jhorzek/lessSEM)

Assuming that our model is set up, we are ready to link everything to [lessSEM](https://github.com/jhorzek/lessSEM). First, create a new file (we called ours [src/optimization.cpp](https://github.com/jhorzek/lessLM/blob/main/src/optimization.cpp)). Here, it is important to include the lessSEM.h headers (see [src/optimization.cpp](https://github.com/jhorzek/lessLM/blob/main/src/optimization.cpp)). All further steps necessary to use the [lessSEM](https://github.com/jhorzek/lessSEM) optimizers are outlined in the comments included in the new file [src/optimization.cpp](https://github.com/jhorzek/lessLM/blob/main/src/optimization.cpp). Open this file and follow the instructions. Therein, we implement both, glmnet and ista optimization for mixed penalties. You can also implement more specialized routines. This is shown in the file [src/optimization_specialized.cpp](https://github.com/jhorzek/lessLM/blob/main/src/optimization_specialized.cpp), where the elastic net penalty and ista optimization of the scad penalty are demonstrated. If you are interested in how the optimization routine is designed, have a look at the vignette "The-optimizer-interface" of the [lessSEM](https://github.com/jhorzek/lessSEM) package. 

## Step 5: Test your function

### Simulating data

```{r}
set.seed(123)
library(lessLM)
library(Matrix) # we will use the matrix package
# to print the matrices below in the same sparse format
# as glmnet or ncvreg

# let's first define a small print function to beautify our results
printCoefficients <- function(model){
  print(t(Matrix:::Matrix(model$B, sparse = TRUE)))
}

# first, we simulate data for our
# linear regression.
N <- 100 # number of persons
p <- 10 # number of predictors
X <- matrix(rnorm(N*p),	nrow = N, ncol = p) # design matrix
b <- c(rep(1,4), 
       rep(0,6)) # true regression weights
y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)

# Our implementation of linear regressions does not automatically add an intercept.
# Therefore, we will add a column for the intercept:
Xext <- cbind(1, X)
```

### Using the functions in [src/optimization.cpp](https://github.com/jhorzek/lessLM/blob/main/src/optimization.cpp)

The implementation in [src/optimization.cpp](https://github.com/jhorzek/lessLM/blob/main/src/optimization.cpp) is very flexible as it allows for combining
different penalties for different parameters. To use the function, we have to first
provide some starting values:
```{r}
startingValues <- rep(0, ncol(Xext))
names(startingValues) <- paste0("b", 0:(ncol(Xext)-1))
```

Now we can regularize our model:
```{r}
# glmnet
lassoGlmnet <- lessLM::penalizeGlmnet(
  y = y,
  X = Xext,
  # We pass our starting values:
  startingValues = startingValues,
  # For each of our values, we have to specify the penalty we want to use.
  # Possible are "none", "cappedL1", "lasso", "lsp", "mcp", or "scad":
  penalty = c("none", # we don't want to regularize the intercept
              # all other 10 regression parameters are regularized with the lasso:
              "lasso", "lasso", "lasso", "lasso", "lasso", 
              "lasso", "lasso", "lasso", "lasso", "lasso"),
  lambda = .3, # the same lambda will be used for all parameters. We can also pass
  # parameter-specific lambda values
  theta = 0, # Theta is not used by any of the penalties, but we still have to pass
  # some values
  initialHessian = matrix(1)
)
```
Note that we get a warning because we did not pass a true Hessian. For our 
simple model, this will still work.

We can check the paramters as follows:
```{r}
Matrix::Matrix(lassoGlmnet$rawParameters,
               sparse = TRUE)
```
In practice, you will want to use multiple $\lambda$ values:

```{r, warning=FALSE}
# iterate over multiple lambdas
initialHessian <- matrix(1)
estimates <- c()
for(lambda in seq(0,1,.1)){
  fit <- lessLM::penalizeGlmnet(
    y = y,
    X = Xext,
    startingValues = startingValues,
    penalty = c("none", 
                "lasso", "lasso", "lasso", "lasso", "lasso", 
                "lasso", "lasso", "lasso", "lasso", "lasso"),
    lambda = lambda, 
    theta = 0,
    initialHessian = initialHessian
  )
  # save estimates
  estimates <- rbind(estimates, fit$rawParameters)
  # update Hessian for next iterations
  initialHessian = fit$Hessian
}
round(Matrix:::Matrix(estimates, sparse = TRUE),3)
```
We can also mix penalties...

```{r, warning=FALSE}
mixedGlmnet <- lessLM::penalizeGlmnet(
  y = y,
  X = Xext,
  # We pass our starting values:
  startingValues = startingValues,
  # For each of our values, we have to specify the penalty we want to use.
  # Possible are "none", "cappedL1", "lasso", "lsp", "mcp", or "scad":
  penalty = c("none", # we don't want to regularize the intercept
              "cappedL1", "cappedL1", "cappedL1", "cappedL1", "cappedL1", 
              "lasso", "lasso", "lasso", "lasso", "lasso"),
  lambda = .3,
  theta = 1, # theta will be used by cappedL1
  initialHessian = matrix(1)
)

round(Matrix:::Matrix(mixedGlmnet$rawParameters, sparse = TRUE),3)
```
... or use the ista optimizer instead:

```{r, warning=FALSE}
mixedIsta <- lessLM::penalizeIsta(
  y = y,
  X = Xext,
  # We pass our starting values:
  startingValues = startingValues,
  # For each of our values, we have to specify the penalty we want to use.
  # Possible are "none", "cappedL1", "lasso", "lsp", "mcp", or "scad":
  penalty = c("none", # we don't want to regularize the intercept
              "cappedL1", "cappedL1", "cappedL1", "cappedL1", "cappedL1", 
              "lasso", "lasso", "lasso", "lasso", "lasso"),
  lambda = .3,
  theta = 1 # theta will be used by cappedL1
)

round(Matrix:::Matrix(mixedIsta$rawParameters, sparse = TRUE),3)
```
Finally, if we just specify a single penalty, the same penalty will be used for
all parameters:

```{r, warning=FALSE}
mixedIsta <- lessLM::penalizeIsta(
  y = y,
  X = Xext,
  # We pass our starting values:
  startingValues = startingValues,
  penalty = "lasso", # lasso will be applied to all parameters (including 
  # the intercept)
  lambda = .3,
  theta = 0
)

round(Matrix:::Matrix(mixedIsta$rawParameters, sparse = TRUE),3)
```
### Specialized Implementations in [src/optimization_specialized.cpp](https://github.com/jhorzek/lessLM/blob/main/src/optimization_specialized.cpp)

Our specialized implementations fit the model for different values of the tuning
parameters.

#### Elastic Net

We will compare our results to those of [glmnet](https://github.com/cran/glmnet).
```{r}
# define the tuning parameters
lambda = seq(1,0,length.out = 5)

lasso1 <- lessLM::elasticNet(y = y,
                             X = Xext,
                             startingValues = startingValues,
                             alpha = 1, # note: glmnet and lessSEM define 
                             # the elastic net differently (lessSEM follows lslx and regsem)
                             # Therefore, you will get different results if you change alpha
                             # when compared to glmnet
                             lambda = lambda
)

# now, let's use the ista optimizer
lasso2 <- lessLM::elasticNetIsta(y = y,
                                 X = Xext,
                                 startingValues = startingValues,
                                 alpha = 1, # note: glmnet and lessSEM define 
                                 # the elastic net differently (lessSEM follows lslx and regsem)
                                 # Therefore, you will get different results if you change alpha
                                 # when compared to glmnet
                                 lambda = lambda)

# For comparison, we will fit the model with the glmnet package:
library(glmnet)
lassoGlmnet <- glmnet(x = X, 
                      y = y, 
                      lambda = lambda,
                      standardize = FALSE)
coef(lassoGlmnet)
printCoefficients(lasso1)
printCoefficients(lasso2)

```

#### Scad

Our functions implementing the scad penalty can be found in [src/optimization_specialized.cpp](https://github.com/jhorzek/lessLM/blob/main/src/optimization_specialized.cpp). 
We will compare our function to that of [ncvreg](https://github.com/pbreheny/ncvreg). Importantly, 
[ncvreg](https://github.com/pbreheny/ncvreg) standardizes 
the data internally. To use exactly the same data set with both packages,
we apply this standardization first. 

```{r}
library(ncvreg)
X <- ncvreg::std(X)
attr(X, "center") <- NULL
attr(X, "scale") <- NULL
attr(X, "nonsingular") <- NULL

Xext <- cbind(1, X)

# Now, let's fit our model with the standardized data
scad1 <- lessLM::scadIsta(y = y, 
                          X = Xext,
                          startingValues = startingValues,
                          theta = 3, 
                          lambda = lambda)

# for comparison, we use ncvreg
scadFit <- ncvreg(X = X, 
                  y = y, 
                  penalty = "SCAD",
                  lambda = lambda, 
                  gamma = 3)

coef(scadFit)
printCoefficients(scad1)
```

# References

* Rosseel, Y. (2012). lavaan: An R Package for Structural Equation Modeling. Journal of Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02
* Breheny, P. (2021). ncvreg: Regularization paths for scad and mcp penalized regression models.
* Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
